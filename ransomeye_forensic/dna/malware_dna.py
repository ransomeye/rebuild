# Path and File Name : /home/ransomeye/rebuild/ransomeye_forensic/dna/malware_dna.py
# Author: nXxBku0CKFAJCBN3X1g3bQk7OxYQylg8CMw1iGsq7gU
# Details of functionality of this file: Extract static and dynamic features from binaries/artifacts to generate malware DNA signatures

import os
import hashlib
import struct
import re
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from datetime import datetime
import logging
import math

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MalwareDNAExtractor:
    """
    Extract features from binaries/artifacts to generate malware DNA signatures.
    Supports PE (Windows), ELF (Linux), and raw binary analysis.
    """
    
    def __init__(self):
        """Initialize malware DNA extractor."""
        self.min_string_length = 4
        self.max_string_length = 256
    
    def extract_dna(self, artifact_path: str, artifact_type: str = 'binary') -> Dict:
        """
        Extract comprehensive DNA signature from artifact.
        
        Args:
            artifact_path: Path to artifact file
            artifact_type: Type of artifact ('binary', 'pe', 'elf', 'memory')
            
        Returns:
            Dictionary with extracted DNA features
        """
        logger.info(f"Extracting DNA from {artifact_path}")
        
        artifact_path_obj = Path(artifact_path)
        if not artifact_path_obj.exists():
            raise FileNotFoundError(f"Artifact not found: {artifact_path}")
        
        # Read artifact
        with open(artifact_path, 'rb') as f:
            data = f.read()
        
        if len(data) == 0:
            raise ValueError("Artifact is empty")
        
        # Extract features
        dna = {
            'artifact_path': str(artifact_path),
            'artifact_type': artifact_type,
            'file_size': len(data),
            'timestamp': datetime.utcnow().isoformat(),
            'hashes': self._extract_hashes(data),
            'entropy': self._calculate_entropy_features(data),
            'strings': self._extract_strings(data),
            'imports': self._extract_imports(data, artifact_type),
            'sections': self._extract_sections(data, artifact_type),
            'api_calls': [],  # Will be populated by sequence extractor
            'metadata': self._extract_metadata(data, artifact_type)
        }
        
        logger.info(f"DNA extraction completed: {len(dna['strings'])} strings, {len(dna.get('imports', []))} imports")
        
        return dna
    
    def _extract_hashes(self, data: bytes) -> Dict[str, str]:
        """Extract various hash algorithms."""
        return {
            'md5': hashlib.md5(data).hexdigest(),
            'sha1': hashlib.sha1(data).hexdigest(),
            'sha256': hashlib.sha256(data).hexdigest(),
            'sha512': hashlib.sha512(data).hexdigest()
        }
    
    def _calculate_entropy_features(self, data: bytes) -> Dict:
        """Calculate entropy features."""
        # Overall entropy
        overall_entropy = self._shannon_entropy(data)
        
        # Sliding window entropy map
        window_size = min(4096, len(data) // 10)
        if window_size < 256:
            window_size = 256
        
        entropy_map = []
        step = window_size // 2
        
        for i in range(0, len(data) - window_size + 1, step):
            window = data[i:i + window_size]
            entropy = self._shannon_entropy(window)
            entropy_map.append({
                'offset': i,
                'entropy': entropy,
                'high_entropy': entropy > 7.0  # Likely packed/encrypted
            })
        
        # Count high entropy regions
        high_entropy_count = sum(1 for e in entropy_map if e['high_entropy'])
        
        return {
            'overall': overall_entropy,
            'max': max((e['entropy'] for e in entropy_map), default=0),
            'min': min((e['entropy'] for e in entropy_map), default=0),
            'avg': sum(e['entropy'] for e in entropy_map) / len(entropy_map) if entropy_map else 0,
            'high_entropy_regions': high_entropy_count,
            'high_entropy_percentage': (high_entropy_count / len(entropy_map) * 100) if entropy_map else 0,
            'map': entropy_map[:100]  # Limit to first 100 for storage
        }
    
    def _shannon_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy."""
        if len(data) == 0:
            return 0.0
        
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1
        
        entropy = 0.0
        data_len = len(data)
        
        for count in byte_counts:
            if count > 0:
                probability = count / data_len
                entropy -= probability * math.log2(probability)
        
        return entropy
    
    def _extract_strings(self, data: bytes) -> List[Dict]:
        """Extract printable strings from binary."""
        strings = []
        
        # Extract ASCII strings
        ascii_pattern = re.compile(rb'[\x20-\x7E]{' + str(self.min_string_length).encode() + rb',' + str(self.max_string_length).encode() + rb'}')
        
        for match in ascii_pattern.finditer(data):
            try:
                string_value = match.group().decode('ascii', errors='ignore')
                if len(string_value) >= self.min_string_length:
                    strings.append({
                        'offset': match.start(),
                        'value': string_value,
                        'length': len(string_value),
                        'type': 'ascii'
                    })
            except:
                pass
        
        # Extract UTF-16 strings (common in Windows binaries)
        utf16_strings = self._extract_utf16_strings(data)
        strings.extend(utf16_strings)
        
        # Sort by offset
        strings.sort(key=lambda x: x['offset'])
        
        # Filter suspicious strings (URLs, IPs, file paths, etc.)
        suspicious_strings = self._filter_suspicious_strings(strings)
        
        return {
            'all': strings[:1000],  # Limit for storage
            'suspicious': suspicious_strings,
            'count': len(strings),
            'suspicious_count': len(suspicious_strings)
        }
    
    def _extract_utf16_strings(self, data: bytes) -> List[Dict]:
        """Extract UTF-16LE strings."""
        strings = []
        
        # Look for UTF-16LE patterns (even-length, null-terminated)
        i = 0
        while i < len(data) - (self.min_string_length * 2):
            if data[i + 1] == 0:  # Potential UTF-16LE start
                string_bytes = bytearray()
                j = i
                while j < len(data) - 1 and data[j + 1] == 0 and data[j] != 0:
                    string_bytes.append(data[j])
                    j += 2
                
                if len(string_bytes) >= self.min_string_length:
                    try:
                        string_value = string_bytes.decode('ascii', errors='ignore')
                        strings.append({
                            'offset': i,
                            'value': string_value,
                            'length': len(string_value),
                            'type': 'utf16'
                        })
                    except:
                        pass
                i = j + 2
            else:
                i += 1
        
        return strings
    
    def _filter_suspicious_strings(self, strings: List[Dict]) -> List[Dict]:
        """Filter strings that look suspicious (URLs, IPs, file paths, etc.)."""
        suspicious = []
        
        url_pattern = re.compile(r'https?://[^\s]+', re.IGNORECASE)
        ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
        path_pattern = re.compile(r'[A-Za-z]:\\[^\s]+|/[^\s]+', re.IGNORECASE)
        registry_pattern = re.compile(r'HKEY_[A-Z_]+\\[^\s]+', re.IGNORECASE)
        
        for s in strings:
            value = s['value']
            is_suspicious = False
            reasons = []
            
            if url_pattern.search(value):
                is_suspicious = True
                reasons.append('url')
            if ip_pattern.search(value):
                is_suspicious = True
                reasons.append('ip')
            if path_pattern.search(value):
                is_suspicious = True
                reasons.append('path')
            if registry_pattern.search(value):
                is_suspicious = True
                reasons.append('registry')
            
            if is_suspicious:
                s['reasons'] = reasons
                suspicious.append(s)
        
        return suspicious
    
    def _extract_imports(self, data: bytes, artifact_type: str) -> List[Dict]:
        """Extract import table hashes (simulated for PE/ELF)."""
        imports = []
        
        if artifact_type == 'pe' or (len(data) > 64 and data[:2] == b'MZ'):
            # PE file - extract DLL imports
            imports = self._extract_pe_imports(data)
        elif artifact_type == 'elf' or (len(data) > 16 and data[:4] == b'\x7fELF'):
            # ELF file - extract library imports
            imports = self._extract_elf_imports(data)
        else:
            # Raw binary - try to detect import-like patterns
            imports = self._extract_generic_imports(data)
        
        return imports
    
    def _extract_pe_imports(self, data: bytes) -> List[Dict]:
        """Extract PE import table (simplified - production would use pefile library)."""
        imports = []
        
        # Simplified PE parsing - in production use pefile or similar
        # Look for common DLL names in strings
        common_dlls = [
            b'kernel32.dll', b'ntdll.dll', b'user32.dll', b'advapi32.dll',
            b'ws2_32.dll', b'msvcrt.dll', b'ole32.dll', b'shell32.dll'
        ]
        
        for dll_name in common_dlls:
            if dll_name.lower() in data.lower():
                imports.append({
                    'dll': dll_name.decode('ascii', errors='ignore').lower(),
                    'type': 'pe_dll',
                    'offset': data.lower().find(dll_name.lower())
                })
        
        # Hash of import section
        if len(imports) > 0:
            import_hash = hashlib.sha256(b''.join([i['dll'].encode() for i in imports])).hexdigest()
            for imp in imports:
                imp['import_hash'] = import_hash
        
        return imports
    
    def _extract_elf_imports(self, data: bytes) -> List[Dict]:
        """Extract ELF library imports (simplified)."""
        imports = []
        
        # Look for common library names
        common_libs = [
            b'libc.so', b'libpthread.so', b'libdl.so', b'libm.so',
            b'libssl.so', b'libcrypto.so', b'libz.so'
        ]
        
        for lib_name in common_libs:
            if lib_name in data:
                imports.append({
                    'library': lib_name.decode('ascii', errors='ignore'),
                    'type': 'elf_lib',
                    'offset': data.find(lib_name)
                })
        
        return imports
    
    def _extract_generic_imports(self, data: bytes) -> List[Dict]:
        """Extract generic import-like patterns."""
        imports = []
        
        # Look for function name patterns
        function_patterns = [
            b'CreateFile', b'ReadFile', b'WriteFile', b'DeleteFile',
            b'RegOpenKey', b'RegSetValue', b'InternetOpen', b'Socket',
            b'execve', b'fork', b'connect', b'bind'
        ]
        
        for pattern in function_patterns:
            if pattern in data:
                imports.append({
                    'function': pattern.decode('ascii', errors='ignore'),
                    'type': 'generic',
                    'offset': data.find(pattern)
                })
        
        return imports
    
    def _extract_sections(self, data: bytes, artifact_type: str) -> List[Dict]:
        """Extract section information."""
        sections = []
        
        # For PE/ELF, would parse actual section headers
        # For now, simulate sections based on entropy
        window_size = min(4096, len(data) // 10)
        if window_size < 256:
            window_size = 256
        
        for i in range(0, len(data), window_size):
            section_data = data[i:i + window_size]
            entropy = self._shannon_entropy(section_data)
            
            sections.append({
                'offset': i,
                'size': len(section_data),
                'entropy': entropy,
                'type': 'high_entropy' if entropy > 7.0 else 'normal'
            })
        
        return sections[:50]  # Limit sections
    
    def _extract_metadata(self, data: bytes, artifact_type: str) -> Dict:
        """Extract file metadata."""
        metadata = {
            'size': len(data),
            'type': artifact_type,
            'is_executable': False,
            'is_packed': False,
            'has_signature': False
        }
        
        # Detect PE
        if len(data) > 64 and data[:2] == b'MZ':
            metadata['is_executable'] = True
            metadata['format'] = 'pe'
        
        # Detect ELF
        elif len(data) > 16 and data[:4] == b'\x7fELF':
            metadata['is_executable'] = True
            metadata['format'] = 'elf'
        
        # Detect high entropy (likely packed)
        overall_entropy = self._shannon_entropy(data)
        if overall_entropy > 7.5:
            metadata['is_packed'] = True
        
        return metadata

